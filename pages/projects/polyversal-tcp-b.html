<?xml version="1.0" encoding="UTF-8"?>


      <title>Polyversal TCP and FABLE</title>
      
  <style>
  blockquote {
    margin-left: 10px;
    padding-left: 20px;
    width: 80%;
    color: #330000;
  } 
  div.changelog { width:80%; font-size:90%; }
  blockquote p { margin-left: 0;  }
  div.ocl p { width: 75%; min-width: 400px; }
  div.ocl a { color: #330099; } 
  .ocl-output {
   width: 75%;
   padding:.4em 0;
   margin:0 0 1em 0;
   border-bottom: 1px solid #cccccc;
  }
  .ocl-cogs span { padding-right:30px; background-size:20px; background: url(../cogs.png) no-repeat right center; }
  .ocl-print span { padding-right:30px; background-size:20px; background: url(../print.png) no-repeat right center; }
  .ocl-cloud span { padding-right:30px; background-size:20px; background: url(../cloud.png) no-repeat right center; }
  .ocl-group span { padding-right:30px; background-size:20px; background: url(../group.png) no-repeat right center; }
  .ocl-bullhorn span { padding-right:30px; background-size:20px; background: url(../bullhorn.png) no-repeat right center; }
  .ocl-asset span { padding-right:30px; background-size:20px; background: url(../wrench.png) no-repeat right center; }
  </style>

      <body>
      <div class="ucampas-toc right"/>
      <div class="ocl">
        <h1 id="Polyversal TCP and FABLE">Polyversal TCP and FABLE</h1>
        <p>Application developers have two choices for sending data over the Internet - TCP, with its reliable, ordered, congestion-controlled byte stream model or UDP with its unordered, unreliable datagram model. As middleboxes have become ubiquitous, it has become impossible for alternative transports to exist and so application developers have come to view opening a TCP socket as the only reliable way to connect to a server. Over time, TCP has become the de facto 'narrow waist' of the Internet - a one-size-fits-all transport that is poorly suited to the need of modern applications. This has made it increasingly hard for novel transport protocols to be deployed. Some recent proposals circumvent this problem by camouflaging new transports so that they appear like TCP to middleboxes. We draw the key lessons from this approach and show how this could lead to a true one-size-fits-all transport: 'Polyversal TCP'.</p>

        <h4 id="Team">Team</h4>
        <ul class="compact">  
        <li><a href="../people/avsm.html">Anil Madhavapeddy</a>, project owner (2012-)</li>
      </ul>
         
         <h4 id="Recent Updates">Recent Updates</h4><ul class="compact"> <li><a class="icon-pdf"/><i>10 Dec 2012</i>: 
    Published  <a class="icon-pdf" href="http://conferences.sigcomm.org/co-next/2012/eproceedings/student/p35.pdf">Evolving TCP. How hard can it be?</a> <i><a href="polyversal-tcp.html#output-conext-workshop">(more)</a></i></li>  <li><a class="icon-pdf"/><i>3 Mar 2012</i>: 
    Published  <a class="icon-pdf" href="http://anil.recoil.org/papers/2012-resolve-fable.pdf">The Case for Reconfigurable I/O Channels</a> <i><a href="polyversal-tcp.html#output-resolve2012">(more)</a></i></li>  <li><a class="icon-video"/><i>4 Feb 2012</i>: 
    Talk on  <a href="https://archive.fosdem.org/2012/schedule/event/unixio.html">The Wild West of UNIX I/O</a> <i><a href="polyversal-tcp.html#output-wildwestunix">(more)</a></i></li>  <li><a class="icon-community"/><i>29 Jan 2012</i>: 
    Press on  <a href="https://archive.fosdem.org/2012/interview/anil-madhavapeddy">Interview with Anil Madhavapeddy</a> <i><a href="polyversal-tcp.html#output-fosdem-interview">(more)</a></i></li> </ul> 
         <h2 id="Publications">Publications</h2> 
        
       <a name="output-conext-workshop"/>
       <h3 class="ocl-output ocl-print" id="Evolving TCP. How hard can it be?"><span>Evolving TCP. How hard can it be?</span></h3>
       <p>Paper on <a class="icon-pdf" href="http://conferences.sigcomm.org/co-next/2012/eproceedings/student/p35.pdf">Evolving TCP. How hard can it be?</a> at <a href="http://conferences.sigcomm.org/co-next/2012/workshops/student/program.html">ACM CoNEXT 2012 Student Workshop</a> with this abstract:</p>
        <blockquote><p>Over the last decade TCP has become the de facto “narrow waist” of the Internet - a one-size-fits-all transport that is poorly suited to the needs of modern applications. As middleboxes have become ubiquitous, it has become nigh im- possible for alternative transports to exist and so application developers have come to view opening a TCP socket as the only reliable way to connect to a server. Some recent propos- als circumvent this problem by camouflaging new transports so that they appear like TCP to middleboxes. We draw the key lessons from this approach and show how this could lead to a true one-size-fits-all transport: “Polyversal TCP”.</p>
</blockquote> 
        
       <div><p><em>10 Dec 2012</em>: Toby Moncaster presented the ongoing PVTCP work at ACM CoNEXT 2012.</p>
</div>
      
       <a name="output-resolve2012"/>
       <h3 class="ocl-output ocl-print" id="The Case for Reconfigurable I/O Channels"><span>The Case for Reconfigurable I/O Channels</span></h3>
       <p>Paper on <a class="icon-pdf" href="http://anil.recoil.org/papers/2012-resolve-fable.pdf">The Case for Reconfigurable I/O Channels</a> at <a href="http://www.dcs.gla.ac.uk/conferences/resolve12">Runtime Environments, Systems, Layering and Virtualized Environments (RESoLVE) 2012</a> with this abstract:</p>
        <blockquote><p>Datacenter environments are increasingly layered, with multicore parallelism, OS virtualisation and NUMA memory all introducing variable latency and throughput for data transmission. For a programmer deploying applications in such a shifting environment, it is unclear how best to use venerable interfaces such as the sockets layer. Kernel hackers realise there is some performance hit to all the software layering, but quantitative figures are hard to find. This is a position paper of two rather different halves. We first seek to understand just how big the impact of NUMA layouts and OS virtualisation have been on I/O performance. To do this, we implemented a variety of IPC mechanisms (from TCP sockets to shared memory) and benchmarked them under modern multicore hardware and Xen. We discovered a large variance in throughput and latency under different scheduling conditions (over an order of magnitude in some cases), and also some rather inexplicable results which point to the extreme difficulty of predicting cross-layer performance. In the second half, we describe the early design of a system which aims to overcome these multiplexed I/O scheduling issues. It provides an efficient, zero-copy data transmission interface that automates the selection of the underlying transport, and the facility to dynamically reconfigure transports as system conditions change. Finally, we discuss the implications of extending the OS with explicit I/O flow tracking - eliminating contention, transparent transport-level security and an upgrade path to multi-path TCP.</p>
</blockquote> 
        <ul> <li><a class="icon-pdf" href="http://anil.recoil.org/talks/2012-resolve-fable.pdf">PDF slides</a></li>  <li><a href="http://fable.io">IPC-Bench dataset</a></li>  <li><a href="http://github.com/avsm/ipc-bench">IPC-Bench code</a></li> </ul> 
       <div><p><em>3 Mar 2012</em>: Anil Madhavapeddy presented this paper at RESoLVE 2012 at the Royal Society in London. There was a good discussion about whether the trend is for more unpredictability (consensus: yes), and whether the FABLE API should be compatible with the sockets API instead. This led us to think about the PVTCP extensions.</p>
</div>
      
         <h2 id="Activity">Activity</h2> 
        
       <a name="output-wildwestunix"/>
       <h3 class="ocl-output ocl-bullhorn" id="The Wild West of UNIX I/O"><span>The Wild West of UNIX I/O</span></h3>
       <p>Talk on <a href="https://archive.fosdem.org/2012/schedule/event/unixio.html">The Wild West of UNIX I/O</a> at <a href="https://archive.fosdem.org/2012">Free and Open Source Software Developers European Meeting (FOSDEM)</a>:</p>
        <blockquote><p>Inter-process communication and remote procedure call facilities have existed in operating systems for many decades. Ever since the first parallel applications ran on time-sharing machines, programmers have sought ways to communicate between processes running on a single machine, and the first networked applications introduced the concept of sending a message to trigger a remote action.</p>
<p>And today, these primitives are more relevant than ever before: parallel programming on clusters of machines relies heavily on facilities to pass data between processes and hosts. On a higher level, data-flow frameworks for parallel processing of large data sets (such as Hadoop or CIEL) depend on passing data between different tasks, which may run anywhere, including local to a machine, on a networked cluster, or far away in a virtualised wide-area cloud.</p>
<p>And yet, we are stuck with UNIX communication APIs closely coupled to the underlying mechanisms used to implement them: the programmer choice of sockets, pipes or shared memory constitutes an implicit choice of a whole set of assumptions about the relative locations of the communicating parties, as well as how the message is to be delivered. Worse even, the implicit trade-offs may not be the same in a different environment, and thus the programmer's choice of API depends on assumptions about the runtime environment (hardware, software and setup) in addition to the characteristics inherent to a mechanism implied.</p>
<p>This talk will firstly discuss the impossibility of using current APIs efficiently (via benchmarks on a diverse set of hardware (from many-core AMDs to the experimental Intel SCC). Finally, I will describe our work on introducing a hierarchical name system and extended socket API that adds support for automatic transport selection and reconfigurable sockets. This permits many NUMA-related optimisations on single hosts, for VMs to switch to shared memory communication if on the same physical host, and for seamless network-wide protocol upgrades to multi path TCP or TCPcrypt.</p>
</blockquote> 
        <ul> <li><a class="icon-pdf" href="http://anil.recoil.org/talks/2012-fosdem-io.pdf">PDF slides</a></li>  <li><a href="http://fable.io">IPC-Bench dataset</a></li>  <li><a href="http://github.com/avsm/ipc-bench">IPC-Bench code</a></li>  <li><a class="icon-video" href="http://video.fosdem.org/2012/maintracks/k.1.105/The_Wild_West_of_UNIX_I_O.webm">Video of talk</a></li> </ul> 
       <div><p><em>4 Feb 2012</em>: Anil Madhavapeddy delivered a keynote session on the topic of UNIX IO and its unpredictability on modern hardware. There were about 2000 people in the audience.</p>
</div>
      
       <a name="output-fosdem-interview"/>
       <h3 class="ocl-output ocl-file" id="Interview with Anil Madhavapeddy"><span>Interview with Anil Madhavapeddy</span></h3>
       <p>Article on <a href="https://archive.fosdem.org/2012/interview/anil-madhavapeddy">Interview with Anil Madhavapeddy</a> at <a href="https://archive.fosdem.org/2012">Free and Open Source Software Developers European Meeting (FOSDEM)</a>: </p>
        <blockquote><p><em>What effect does a virtual environment have on the performance of the traditional UNIX communication mechanisms?</em></p>
<p>Quite a dramatic effect, some due to the limitations of hardware, and others due to the Xen domain scheduler needing to be a little smarter (something that various developers at Citrix are furiously hacking on as we speak!).</p>
<p>The biggest problem is that the combination of VM scheduling (in Xen), with process scheduling (in the guest VM) makes most operations much more latent than when running on native. Similarly, 64-bit VMs must jump through the hypervisor <em>and</em> kernel when performing a system call, due to the lack of segmentation protection (which is what is used in 32-bit Xen to protect the hypervisor from the guest kernel and userspace). The result is that some operations which have a certain performance/safety tradeoff in native are much more skewed when virtualised. Thus, the choice of IPC mechanism also changes accordingly when virtualised.</p>
</blockquote> 
        
       <div><p><em>29 Jan 2012</em>: Anil Madhavapeddy explains the background to FABLE and PVTCP, and why we need to move past the venerable POSIX socket interface.</p>
</div>
      
         
        
      </div>
      </body>
  